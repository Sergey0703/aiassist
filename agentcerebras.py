import asyncio
import logging
import os
import base64
import time
from dotenv import load_dotenv
from livekit.agents import (
    Agent,
    AgentSession,
    JobContext,
    WorkerOptions,
    cli,
    AutoSubscribe,
    get_job_context,
)
from livekit.plugins import openai, silero, google, assemblyai
from livekit.agents.llm import ChatContext, ChatMessage, ImageContent
from livekit.agents.utils.images import encode, EncodeOptions, ResizeOptions
from livekit import rtc

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã: –ø–æ–≥–æ–¥–∞, –ø–æ–∏—Å–∫ –∏ email
from toolscerebras import get_weather, search_web, send_email, test_cerebras

# -------------------- Setup --------------------
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ
logger = logging.getLogger("cerebras-assistant")
logger.setLevel(logging.INFO)

formatter = logging.Formatter(
    "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)

console_handler = logging.StreamHandler()
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

file_handler = logging.FileHandler("cerebras_agent.log", encoding='utf-8')
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)

# –ü–æ–ª—É—á–∞–µ–º API –∫–ª—é—á–∏
openai_api_key = os.getenv("OPENAI_API_KEY")
google_api_key = os.getenv("GOOGLE_API_KEY")
enable_video = os.getenv("ENABLE_VIDEO", "true").lower() == "true"

if not openai_api_key:
    logger.error("OPENAI_API_KEY not found in environment variables")
    raise ValueError("OPENAI_API_KEY is required")

if enable_video and not google_api_key:
    logger.error("GOOGLE_API_KEY not found but video is enabled")
    raise ValueError("GOOGLE_API_KEY is required when ENABLE_VIDEO=true")

logger.info(f"üé¨ [CONFIG] Video processing: {'ENABLED' if enable_video else 'DISABLED'}")

# -------------------- Video Helper Functions (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ) --------------------
def encode_frame_to_base64(frame: rtc.VideoFrame) -> str:
    """–ö–æ–¥–∏—Ä—É–µ—Ç –≤–∏–¥–µ–æ –∫–∞–¥—Ä –≤ base64 JPEG –¥–ª—è Gemini"""
    try:
        image_bytes = encode(
            frame,
            EncodeOptions(
                format="JPEG",
                quality=70,
                resize_options=ResizeOptions(
                    width=512,
                    height=512,
                    strategy="scale_aspect_fit"
                )
            )
        )
        
        base64_str = base64.b64encode(image_bytes).decode('utf-8')
        return f"data:image/jpeg;base64,{base64_str}"
        
    except Exception as e:
        logger.error(f"‚ùå [VIDEO ENCODE] Error encoding frame: {e}")
        return None

# -------------------- Agent Class (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è) --------------------
class CerebrasHybridAssistant(Agent):
    """–ü—Ä–æ—Å—Ç–æ–π –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫: OpenAI –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–π —Ä–∞–±–æ—Ç—ã + –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ Gemini"""
    
    def __init__(self):
        super().__init__(
            instructions=(
                "You are a helpful voice assistant. "
                "ALWAYS respond in English only. "
                "Be EXTREMELY concise - use MAXIMUM 1-2 short sentences. NEVER more than 20 words total. "
                "When users ask about weather, use get_weather tool. "
                "When users ask for web info, use search_web tool. "
                "When users ask to send email, use send_email tool. "
                f"{'You have access to live video analysis. ' if enable_video else ''}"
                "When video information is available, incorporate it naturally into responses. "
                "Be specific when describing what you see. "
                "Do NOT add phrases like 'How can I help' - just answer directly and stop."
            ),
            tools=[] #tools=[get_weather, search_web, send_email, test_cerebras],
        )
        
        # –í–∏–¥–µ–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        self._latest_frame = None
        self._video_stream = None
        self._frame_count = 0
        self._last_frame_time = 0
        self._video_tasks = []
        self._gemini_llm = None
        self._latest_video_description = None
        
        logger.info(f"‚úÖ Cerebras Assistant initialized (Video: {'ON' if enable_video else 'OFF'})")

    async def on_enter(self):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –∫–æ–≥–¥–∞ –∞–≥–µ–Ω—Ç –≤—Ö–æ–¥–∏—Ç –≤ –∫–æ–º–Ω–∞—Ç—É"""
        logger.info("üöÄ [AGENT] Agent entered room")
        
        if enable_video:
            logger.info("üìπ [VIDEO] Setting up video processing...")
            try:
                self._gemini_llm = google.LLM(
                    model="gemini-1.5-flash",
                    temperature=0.1,
                )
                logger.info("‚úÖ [GEMINI] LLM created for video analysis")
            except Exception as e:
                logger.error(f"‚ùå [GEMINI] Failed to create LLM: {e}")
                return
            
            await self._setup_video_processing()
            
            room = get_job_context().room
            
            @room.on("track_subscribed")
            def on_track_subscribed(track: rtc.Track, publication: rtc.RemoteTrackPublication, participant: rtc.RemoteParticipant):
                if track.kind == rtc.TrackKind.KIND_VIDEO:
                    logger.info(f"üìπ [VIDEO] New video track from {participant.identity}")
                    asyncio.create_task(self._setup_video_stream(track))
        else:
            logger.info("üìπ [VIDEO] Video processing disabled")

    async def _setup_video_processing(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ"""
        if not enable_video:
            return
            
        try:
            room = get_job_context().room
            
            for participant in room.remote_participants.values():
                logger.info(f"üë§ [PARTICIPANT] Checking {participant.identity} for video tracks")
                
                for publication in participant.track_publications.values():
                    track = publication.track
                    if track and track.kind == rtc.TrackKind.KIND_VIDEO:
                        logger.info(f"üìπ [VIDEO] Found existing video track from {participant.identity}")
                        await self._setup_video_stream(track)
                        return
            
            logger.info("üìπ [VIDEO] No existing video tracks found, waiting for new ones...")
            
        except Exception as e:
            logger.error(f"‚ùå [VIDEO SETUP] Error: {e}")

    async def _setup_video_stream(self, track: rtc.Track):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ—Ç–æ–∫–∞ –¥–ª—è –≤–∏–¥–µ–æ"""
        if not enable_video:
            return
            
        try:
            if self._video_stream:
                logger.info("üìπ [VIDEO] Closing previous video stream")
                self._video_stream.close()
            
            self._video_stream = rtc.VideoStream(track)
            logger.info("üìπ [VIDEO] Created new video stream")
            
            task = asyncio.create_task(self._process_video_frames())
            self._video_tasks.append(task)
            task.add_done_callback(lambda t: self._video_tasks.remove(t) if t in self._video_tasks else None)
            
            logger.info("‚úÖ [VIDEO] Video stream processing started")
            
        except Exception as e:
            logger.error(f"‚ùå [VIDEO STREAM] Error: {e}")

    async def _process_video_frames(self):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∏–¥–µ–æ –∫–∞–¥—Ä—ã —á–µ—Ä–µ–∑ Gemini"""
        if not enable_video:
            return
            
        try:
            logger.info("üé¨ [VIDEO] Starting Gemini video analysis loop")
            
            async for event in self._video_stream:
                try:
                    frame = event.frame
                    self._frame_count += 1
                    
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π 60-–π –∫–∞–¥—Ä
                    if self._frame_count % 60 != 0:
                        continue
                    
                    if self._frame_count % 60 == 0:
                        logger.info(f"üì∏ [VIDEO] Processing frame {self._frame_count}")
                    
                    encoded_frame = encode_frame_to_base64(frame)
                    
                    if encoded_frame:
                        self._latest_frame = encoded_frame
                        asyncio.create_task(self._analyze_frame_with_gemini(encoded_frame))
                        self._last_frame_time = time.time()
                        logger.info(f"‚úÖ [VIDEO] Sent frame {self._frame_count} to Gemini")
                        
                except Exception as e:
                    logger.error(f"‚ùå [VIDEO] Error processing frame {self._frame_count}: {e}")
                    
        except Exception as e:
            logger.error(f"‚ùå [VIDEO] Video loop ended: {e}")
        
        logger.info("üõë [VIDEO] Gemini video analysis loop ended")

    async def _analyze_frame_with_gemini(self, encoded_frame: str):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∏–¥–µ–æ –∫–∞–¥—Ä —á–µ—Ä–µ–∑ Gemini"""
        if not enable_video or not self._gemini_llm:
            return
            
        try:
            video_context = ChatContext()
            image_content = ImageContent(image=encoded_frame)
            
            video_context.append(
                role="user",
                text="Analyze this image briefly. Describe what you see in 10 words or less. Focus on people, objects, actions.",
                images=[image_content]
            )
            
            logger.info("üß† [GEMINI] Sending frame for analysis...")
            
            chat_stream = self._gemini_llm.chat(chat_ctx=video_context)
            
            response_text = ""
            async for chunk in chat_stream:
                if chunk.text:
                    response_text += chunk.text
            
            if response_text:
                self._latest_video_description = response_text.strip()
                logger.info(f"‚úÖ [GEMINI] Video analysis: '{self._latest_video_description}'")
            else:
                logger.warning("‚ö†Ô∏è [GEMINI] No response from Gemini")
                
        except Exception as e:
            logger.error(f"‚ùå [GEMINI] Error in video analysis: {e}")
            self._latest_video_description = None

    async def on_user_turn_completed(self, turn_ctx: ChatContext, new_message: ChatMessage) -> None:
        """–î–æ–±–∞–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –≤–∏–¥–µ–æ –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É"""
        if not enable_video:
            return
            
        try:
            if self._latest_video_description:
                frame_age = time.time() - self._last_frame_time
                
                if frame_age < 10:
                    if hasattr(new_message, 'content') and isinstance(new_message.content, list):
                        video_context = f"[Video context: {self._latest_video_description}]"
                        new_message.content.append(video_context)
                        logger.info(f"üìπ [HYBRID] Added video context: '{self._latest_video_description}'")
                    else:
                        logger.warning("‚ö†Ô∏è [HYBRID] Could not add video description")
                else:
                    logger.warning(f"‚ö†Ô∏è [HYBRID] Video description too old ({frame_age:.1f}s)")
            else:
                logger.info("üìπ [HYBRID] No video description available")
                
        except Exception as e:
            logger.error(f"‚ùå [HYBRID] Error adding video description: {e}")

# -------------------- Entrypoint (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π) --------------------
async def entrypoint(ctx: JobContext):
    """–ì–ª–∞–≤–Ω–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞"""
    
    logger.info("üöÄ Starting Cerebras Assistant entrypoint")
    
    await ctx.connect(auto_subscribe=AutoSubscribe.SUBSCRIBE_ALL)
    logger.info(f"‚úÖ Connected to room: {ctx.room.name}")
    
    agent = CerebrasHybridAssistant()
    
    # –û–ë–´–ß–ù–ê–Ø —Å–µ—Å—Å–∏—è –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ, –Ω–æ –ø–æ–∑–∂–µ –∑–∞–º–µ–Ω–∏–º LLM –Ω–∞ Cerebras
    session = AgentSession(
        vad=silero.VAD.load(),
        
        stt=openai.STT(language="en",),
        #stt=assemblyai.STT(),
        
        # –ü–æ–∫–∞ –æ—Å—Ç–∞–≤–ª—è–µ–º OpenAI LLM (–ø–æ–∑–∂–µ –∑–∞–º–µ–Ω–∏–º –Ω–∞ Cerebras)
        #llm=openai.LLM(
        #    model="gpt-4o-mini",
        #    temperature=0.2,
        #),
       llm=openai.LLM(
            model="llama-3.1-8b",
            temperature=0.2,
            base_url="https://api.cerebras.ai/v1",  # Cerebras endpoint
            api_key=os.getenv("CEREBRAS_API_KEY"),
            #tool_choice="auto",
        ),
        
        tts=openai.TTS(
            voice="alloy",
            speed=1.2,
            model="tts-1-hd",
        ),
    )
    
    video_status = "Gemini (video analysis)" if enable_video else "DISABLED"
    logger.info(f"‚úÖ Session created: Whisper STT + OpenAI LLM + {video_status} + TTS + 3 Tools")
    
    # –°–æ–±—ã—Ç–∏—è LiveKit (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
    @session.on("user_input_transcribed")
    def on_user_input_transcribed(event):
        transcript = getattr(event, 'transcript', 'No transcript')
        is_final = getattr(event, 'is_final', False)
        if is_final:
            logger.info(f"üë§ [USER FINAL] {transcript}")
            print(f"\nüë§ [USER] {transcript}")
    
    @session.on("conversation_item_added")
    def on_conversation_item_added(event):
        item = getattr(event, 'item', None)
        if item:
            role = getattr(item, 'role', 'unknown')
            content = getattr(item, 'text_content', '') or str(getattr(item, 'content', ''))
            
            if role == "user":
                logger.info(f"üí¨ [CHAT USER] {content}")
                print(f"üí¨ [CHAT USER] {content}")
            elif role == "assistant":
                logger.info(f"üí¨ [CHAT ASSISTANT] {content}")
                print(f"üí¨ [CHAT ASSISTANT] {content}")
                print("-" * 60)
    
    @session.on("speech_created")
    def on_speech_created(event):
        logger.info("üîä [SPEECH CREATED] Agent is about to speak")
        print("üîä [ASSISTANT] Creating speech...")
    
    @session.on("agent_state_changed")
    def on_agent_state_changed(event):
        old_state = getattr(event, 'old_state', 'unknown')
        new_state = getattr(event, 'new_state', 'unknown')
        logger.info(f"üîÑ [AGENT STATE] {old_state} -> {new_state}")
        print(f"üîÑ [AGENT] {old_state} -> {new_state}")
    
    @session.on("function_tools_executed")
    def on_function_tools_executed(event):
        logger.info("üõ†Ô∏è [TOOLS EXECUTED] Function tools completed")
        print("üõ†Ô∏è [TOOLS] Function executed - processing result...")
        
    @session.on("error")
    def on_error(event):
        error = getattr(event, 'error', str(event))
        logger.error(f"‚ùå [ERROR] {error}")
        print(f"‚ùå [ERROR] {error}")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Å—Å–∏—é
    await session.start(
        agent=agent,
        room=ctx.room,
    )
    
    logger.info("‚úÖ Session started successfully")
    
    # –ù–∞—á–∞–ª—å–Ω–æ–µ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ
    try:
        greeting = f"Say hello briefly as a voice assistant{'with video vision' if enable_video else ''}."
        await session.generate_reply(instructions=greeting)
        logger.info("‚úÖ Initial greeting generated")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not generate initial greeting: {e}")
        greeting_text = f"Hello! I'm your voice assistant{' with video vision' if enable_video else ''}."
        print(f"ü§ñ [ASSISTANT] {greeting_text}")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    print("\n" + "="*80)
    print("ü§ñ [ASSISTANT] Simplified version working!")
    print(f"üìã [INFO] OpenAI STT + OpenAI LLM (temp) + {video_status} + OpenAI TTS")
    print("üîç [VAD] Silero VAD for speech detection")
    print("üåç [STT] English only")
    if enable_video:
        print("üìπ [VIDEO] Gemini video analysis enabled")
    else:
        print("üìπ [VIDEO] DISABLED")
    print("üõ†Ô∏è [TOOLS] Weather, Web Search, and Email available")
    print("")
    print("üéØ [TEST COMMANDS]:")
    print("   ‚Ä¢ 'What's the weather in London?'")
    print("   ‚Ä¢ 'Search for latest AI news'")
    print("   ‚Ä¢ 'Send email to test@example.com'")
    if enable_video:
        print("   ‚Ä¢ 'What do you see?'")
    print("")
    print("üéÆ [CONTROLS] Speak into microphone, Ctrl+C to quit")
    print("="*80 + "\n")
    print("üéôÔ∏è [LISTENING] Start speaking now...")
    
    # –ë–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π —Ü–∏–∫–ª
    try:
        while True:
            await asyncio.sleep(1)
    except KeyboardInterrupt:
        logger.info("üëã [SHUTDOWN] Assistant shutting down...")
        print("\nüëã [ASSISTANT] Goodbye!")

# -------------------- Main --------------------
if __name__ == "__main__":
    logger.info("üöÄ Starting Cerebras Assistant application")
    cli.run_app(
        WorkerOptions(
            entrypoint_fnc=entrypoint
        )
    )