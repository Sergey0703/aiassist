import asyncio
import logging
import os
import base64
import time
from dotenv import load_dotenv
from livekit.agents import (
    Agent,
    AgentSession,
    JobContext,
    WorkerOptions,
    cli,
    AutoSubscribe,
    get_job_context,
)
from livekit.plugins import openai, silero, google, assemblyai
from livekit.agents.llm import ChatContext, ChatMessage, ImageContent
from livekit.agents.utils.images import encode, EncodeOptions, ResizeOptions
from livekit import rtc

# Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ²ÑĞµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹: Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ğ°, Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ email
from toolscerebras import get_weather, search_web, send_email, test_cerebras

# -------------------- Setup --------------------
load_dotenv()

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº Ğ² Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»Ğµ
logger = logging.getLogger("cerebras-assistant")
logger.setLevel(logging.INFO)

formatter = logging.Formatter(
    "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)

console_handler = logging.StreamHandler()
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

file_handler = logging.FileHandler("cerebras_agent.log", encoding='utf-8')
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)

# ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ API ĞºĞ»ÑÑ‡Ğ¸
openai_api_key = os.getenv("OPENAI_API_KEY")
google_api_key = os.getenv("GOOGLE_API_KEY")
enable_video = os.getenv("ENABLE_VIDEO", "true").lower() == "true"

if not openai_api_key:
    logger.error("OPENAI_API_KEY not found in environment variables")
    raise ValueError("OPENAI_API_KEY is required")

if enable_video and not google_api_key:
    logger.error("GOOGLE_API_KEY not found but video is enabled")
    raise ValueError("GOOGLE_API_KEY is required when ENABLE_VIDEO=true")

logger.info(f"ğŸ¬ [CONFIG] Video processing: {'ENABLED' if enable_video else 'DISABLED'}")

# -------------------- Video Helper Functions (ĞºĞ°Ğº Ğ² Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»Ğµ) --------------------
def encode_frame_to_base64(frame: rtc.VideoFrame) -> str:
    """ĞšĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğ´Ñ€ Ğ² base64 JPEG Ğ´Ğ»Ñ Gemini"""
    try:
        image_bytes = encode(
            frame,
            EncodeOptions(
                format="JPEG",
                quality=70,
                resize_options=ResizeOptions(
                    width=512,
                    height=512,
                    strategy="scale_aspect_fit"
                )
            )
        )
        
        base64_str = base64.b64encode(image_bytes).decode('utf-8')
        return f"data:image/jpeg;base64,{base64_str}"
        
    except Exception as e:
        logger.error(f"âŒ [VIDEO ENCODE] Error encoding frame: {e}")
        return None

# -------------------- Agent Class (ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ) --------------------
class CerebrasHybridAssistant(Agent):
    """ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº: OpenAI Ğ´Ğ»Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ + Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Gemini"""
    
    def __init__(self):
        super().__init__(
            instructions=(
                "You are a helpful voice assistant. "
                "ALWAYS respond in English only. "
                "Be EXTREMELY concise - use MAXIMUM 1-2 short sentences. NEVER more than 20 words total. "
                "When users ask about weather, use get_weather tool. "
                "When users ask for web info, use search_web tool. "
                "When users ask to send email, use send_email tool. "
                f"{'You have access to live video analysis. ' if enable_video else ''}"
                "When video information is available, incorporate it naturally into responses. "
                "Be specific when describing what you see. "
                "Do NOT add phrases like 'How can I help' - just answer directly and stop."
            ),
            tools=[] #tools=[get_weather, search_web, send_email, test_cerebras],
        )
        
        # Ğ’Ğ¸Ğ´ĞµĞ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)
        self._latest_frame = None
        self._video_stream = None
        self._frame_count = 0
        self._last_frame_time = 0
        self._video_tasks = []
        self._gemini_llm = None
        self._latest_video_description = None
        
        logger.info(f"âœ… Cerebras Assistant initialized (Video: {'ON' if enable_video else 'OFF'})")

    async def on_enter(self):
        """Ğ’Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ ĞºĞ¾Ğ³Ğ´Ğ° Ğ°Ğ³ĞµĞ½Ñ‚ Ğ²Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² ĞºĞ¾Ğ¼Ğ½Ğ°Ñ‚Ñƒ"""
        logger.info("ğŸš€ [AGENT] Agent entered room")
        
        if enable_video:
            logger.info("ğŸ“¹ [VIDEO] Setting up video processing...")
            try:
                self._gemini_llm = google.LLM(
                    model="gemini-1.5-flash",
                    temperature=0.1,
                )
                logger.info("âœ… [GEMINI] LLM created for video analysis")
            except Exception as e:
                logger.error(f"âŒ [GEMINI] Failed to create LLM: {e}")
                return
            
            await self._setup_video_processing()
            
            room = get_job_context().room
            
            @room.on("track_subscribed")
            def on_track_subscribed(track: rtc.Track, publication: rtc.RemoteTrackPublication, participant: rtc.RemoteParticipant):
                if track.kind == rtc.TrackKind.KIND_VIDEO:
                    logger.info(f"ğŸ“¹ [VIDEO] New video track from {participant.identity}")
                    asyncio.create_task(self._setup_video_stream(track))
        else:
            logger.info("ğŸ“¹ [VIDEO] Video processing disabled")

    async def _setup_video_processing(self):
        """ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾"""
        if not enable_video:
            return
            
        try:
            room = get_job_context().room
            
            for participant in room.remote_participants.values():
                logger.info(f"ğŸ‘¤ [PARTICIPANT] Checking {participant.identity} for video tracks")
                
                for publication in participant.track_publications.values():
                    track = publication.track
                    if track and track.kind == rtc.TrackKind.KIND_VIDEO:
                        logger.info(f"ğŸ“¹ [VIDEO] Found existing video track from {participant.identity}")
                        await self._setup_video_stream(track)
                        return
            
            logger.info("ğŸ“¹ [VIDEO] No existing video tracks found, waiting for new ones...")
            
        except Exception as e:
            logger.error(f"âŒ [VIDEO SETUP] Error: {e}")

    async def _setup_video_stream(self, track: rtc.Track):
        """ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾"""
        if not enable_video:
            return
            
        try:
            if self._video_stream:
                logger.info("ğŸ“¹ [VIDEO] Closing previous video stream")
                self._video_stream.close()
            
            self._video_stream = rtc.VideoStream(track)
            logger.info("ğŸ“¹ [VIDEO] Created new video stream")
            
            task = asyncio.create_task(self._process_video_frames())
            self._video_tasks.append(task)
            task.add_done_callback(lambda t: self._video_tasks.remove(t) if t in self._video_tasks else None)
            
            logger.info("âœ… [VIDEO] Video stream processing started")
            
        except Exception as e:
            logger.error(f"âŒ [VIDEO STREAM] Error: {e}")

    async def _process_video_frames(self):
        """ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğ´Ñ€Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Gemini"""
        if not enable_video:
            return
            
        try:
            logger.info("ğŸ¬ [VIDEO] Starting Gemini video analysis loop")
            
            async for event in self._video_stream:
                try:
                    frame = event.frame
                    self._frame_count += 1
                    
                    # ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ 60-Ğ¹ ĞºĞ°Ğ´Ñ€
                    if self._frame_count % 60 != 0:
                        continue
                    
                    if self._frame_count % 60 == 0:
                        logger.info(f"ğŸ“¸ [VIDEO] Processing frame {self._frame_count}")
                    
                    encoded_frame = encode_frame_to_base64(frame)
                    
                    if encoded_frame:
                        self._latest_frame = encoded_frame
                        asyncio.create_task(self._analyze_frame_with_gemini(encoded_frame))
                        self._last_frame_time = time.time()
                        logger.info(f"âœ… [VIDEO] Sent frame {self._frame_count} to Gemini")
                        
                except Exception as e:
                    logger.error(f"âŒ [VIDEO] Error processing frame {self._frame_count}: {e}")
                    
        except Exception as e:
            logger.error(f"âŒ [VIDEO] Video loop ended: {e}")
        
        logger.info("ğŸ›‘ [VIDEO] Gemini video analysis loop ended")

    async def _analyze_frame_with_gemini(self, encoded_frame: str):
        """ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğ´Ñ€ Ñ‡ĞµÑ€ĞµĞ· Gemini"""
        if not enable_video or not self._gemini_llm:
            return
            
        try:
            video_context = ChatContext()
            image_content = ImageContent(image=encoded_frame)
            
            video_context.append(
                role="user",
                text="Analyze this image briefly. Describe what you see in 10 words or less. Focus on people, objects, actions.",
                images=[image_content]
            )
            
            logger.info("ğŸ§  [GEMINI] Sending frame for analysis...")
            
            chat_stream = self._gemini_llm.chat(chat_ctx=video_context)
            
            response_text = ""
            async for chunk in chat_stream:
                if chunk.text:
                    response_text += chunk.text
            
            if response_text:
                self._latest_video_description = response_text.strip()
                logger.info(f"âœ… [GEMINI] Video analysis: '{self._latest_video_description}'")
            else:
                logger.warning("âš ï¸ [GEMINI] No response from Gemini")
                
        except Exception as e:
            logger.error(f"âŒ [GEMINI] Error in video analysis: {e}")
            self._latest_video_description = None

    async def on_user_turn_completed(self, turn_ctx: ChatContext, new_message: ChatMessage) -> None:
        """Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ"""
        if not enable_video:
            return
            
        try:
            if self._latest_video_description:
                frame_age = time.time() - self._last_frame_time
                
                if frame_age < 10:
                    if hasattr(new_message, 'content') and isinstance(new_message.content, list):
                        video_context = f"[Video context: {self._latest_video_description}]"
                        new_message.content.append(video_context)
                        logger.info(f"ğŸ“¹ [HYBRID] Added video context: '{self._latest_video_description}'")
                    else:
                        logger.warning("âš ï¸ [HYBRID] Could not add video description")
                else:
                    logger.warning(f"âš ï¸ [HYBRID] Video description too old ({frame_age:.1f}s)")
            else:
                logger.info("ğŸ“¹ [HYBRID] No video description available")
                
        except Exception as e:
            logger.error(f"âŒ [HYBRID] Error adding video description: {e}")

# -------------------- Entrypoint (ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¹) --------------------
async def entrypoint(ctx: JobContext):
    """Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ñ‚Ğ¾Ñ‡ĞºĞ° Ğ²Ñ…Ğ¾Ğ´Ğ°"""
    
    logger.info("ğŸš€ Starting Cerebras Assistant entrypoint")
    
    await ctx.connect(auto_subscribe=AutoSubscribe.SUBSCRIBE_ALL)
    logger.info(f"âœ… Connected to room: {ctx.room.name}")
    
    agent = CerebrasHybridAssistant()
    
    # ĞĞ‘Ğ«Ğ§ĞĞĞ¯ ÑĞµÑÑĞ¸Ñ ĞºĞ°Ğº Ğ² Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»Ğµ, Ğ½Ğ¾ Ğ¿Ğ¾Ğ·Ğ¶Ğµ Ğ·Ğ°Ğ¼ĞµĞ½Ğ¸Ğ¼ LLM Ğ½Ğ° Cerebras
    session = AgentSession(
        vad=silero.VAD.load(),
        
        stt=openai.STT(language="en",),
        #stt=assemblyai.STT(),
        
        # ĞŸĞ¾ĞºĞ° Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ OpenAI LLM (Ğ¿Ğ¾Ğ·Ğ¶Ğµ Ğ·Ğ°Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğ½Ğ° Cerebras)
        #llm=openai.LLM(
        #    model="gpt-4o-mini",
        #    temperature=0.2,
        #),
       llm=openai.LLM(
            model="llama-3.1-8b",
            temperature=0.2,
            base_url="https://api.cerebras.ai/v1",  # Cerebras endpoint
            api_key=os.getenv("CEREBRAS_API_KEY"),
            #tool_choice="auto",
        ),
        
        tts=openai.TTS(
            voice="alloy",
            speed=1.2,
            model="tts-1-hd",
        ),
    )
    
    video_status = "Gemini (video analysis)" if enable_video else "DISABLED"
    logger.info(f"âœ… Session created: Whisper STT + OpenAI LLM + {video_status} + TTS + 3 Tools")
    
    # Ğ¡Ğ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ LiveKit (ĞºĞ°Ğº Ğ² Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»Ğµ)
    @session.on("user_input_transcribed")
    def on_user_input_transcribed(event):
        transcript = getattr(event, 'transcript', 'No transcript')
        is_final = getattr(event, 'is_final', False)
        if is_final:
            logger.info(f"ğŸ‘¤ [USER FINAL] {transcript}")
            print(f"\nğŸ‘¤ [USER] {transcript}")
    
    @session.on("conversation_item_added")
    def on_conversation_item_added(event):
        item = getattr(event, 'item', None)
        if item:
            role = getattr(item, 'role', 'unknown')
            content = getattr(item, 'text_content', '') or str(getattr(item, 'content', ''))
            
            if role == "user":
                logger.info(f"ğŸ’¬ [CHAT USER] {content}")
                print(f"ğŸ’¬ [CHAT USER] {content}")
            elif role == "assistant":
                logger.info(f"ğŸ’¬ [CHAT ASSISTANT] {content}")
                print(f"ğŸ’¬ [CHAT ASSISTANT] {content}")
                print("-" * 60)
    
    @session.on("speech_created")
    def on_speech_created(event):
        logger.info("ğŸ”Š [SPEECH CREATED] Agent is about to speak")
        print("ğŸ”Š [ASSISTANT] Creating speech...")
    
    @session.on("agent_state_changed")
    def on_agent_state_changed(event):
        old_state = getattr(event, 'old_state', 'unknown')
        new_state = getattr(event, 'new_state', 'unknown')
        logger.info(f"ğŸ”„ [AGENT STATE] {old_state} -> {new_state}")
        print(f"ğŸ”„ [AGENT] {old_state} -> {new_state}")
    
    @session.on("function_tools_executed")
    def on_function_tools_executed(event):
        logger.info("ğŸ› ï¸ [TOOLS EXECUTED] Function tools completed")
        print("ğŸ› ï¸ [TOOLS] Function executed - processing result...")
        
    @session.on("error")
    def on_error(event):
        error = getattr(event, 'error', str(event))
        logger.error(f"âŒ [ERROR] {error}")
        print(f"âŒ [ERROR] {error}")
    
    # Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ ÑĞµÑÑĞ¸Ñ
    await session.start(
        agent=agent,
        room=ctx.room,
    )
    
    logger.info("âœ… Session started successfully")
    
    # ĞĞ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ
    try:
        greeting = f"Say hello briefly as a voice assistant{'with video vision' if enable_video else ''}."
        await session.generate_reply(instructions=greeting)
        logger.info("âœ… Initial greeting generated")
    except Exception as e:
        logger.warning(f"âš ï¸ Could not generate initial greeting: {e}")
        greeting_text = f"Hello! I'm your voice assistant{' with video vision' if enable_video else ''}."
        print(f"ğŸ¤– [ASSISTANT] {greeting_text}")
    
    # Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ
    print("\n" + "="*80)
    print("ğŸ¤– [ASSISTANT] Simplified version working!")
    print(f"ğŸ“‹ [INFO] OpenAI STT + OpenAI LLM (temp) + {video_status} + OpenAI TTS")
    print("ğŸ” [VAD] Silero VAD for speech detection")
    print("ğŸŒ [STT] English only")
    if enable_video:
        print("ğŸ“¹ [VIDEO] Gemini video analysis enabled")
    else:
        print("ğŸ“¹ [VIDEO] DISABLED")
    print("ğŸ› ï¸ [TOOLS] Weather, Web Search, and Email available")
    print("")
    print("ğŸ¯ [TEST COMMANDS]:")
    print("   â€¢ 'What's the weather in London?'")
    print("   â€¢ 'Search for latest AI news'")
    print("   â€¢ 'Send email to test@example.com'")
    if enable_video:
        print("   â€¢ 'What do you see?'")
    print("")
    print("ğŸ® [CONTROLS] Speak into microphone, Ctrl+C to quit")
    print("="*80 + "\n")
    print("ğŸ™ï¸ [LISTENING] Start speaking now...")
    
    # Ğ‘ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ»
    try:
        while True:
            await asyncio.sleep(1)
    except KeyboardInterrupt:
        logger.info("ğŸ‘‹ [SHUTDOWN] Assistant shutting down...")
        print("\nğŸ‘‹ [ASSISTANT] Goodbye!")

# -------------------- Main --------------------
if __name__ == "__main__":
    logger.info("ğŸš€ Starting Cerebras Assistant application")
    cli.run_app(
        WorkerOptions(
            entrypoint_fnc=entrypoint
        )
    )