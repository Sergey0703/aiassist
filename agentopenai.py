import asyncio
import logging
import os
import base64
from dotenv import load_dotenv
from livekit.agents import (
    Agent,
    AgentSession,
    JobContext,
    WorkerOptions,
    cli,
    AutoSubscribe,
    get_job_context,
)
from livekit.plugins import openai, silero
from livekit.agents.llm import ChatContext, ChatMessage
from livekit.agents.utils.images import encode, EncodeOptions, ResizeOptions
from livekit import rtc

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ —Ç—Ä–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞: –ø–æ–≥–æ–¥–∞, –ø–æ–∏—Å–∫ –∏ email
from aitools import get_weather, search_web, send_email

# -------------------- Setup --------------------
load_dotenv()

# –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–∞—Ö LiveKit
logger = logging.getLogger("openai-assistant")
logger.setLevel(logging.INFO)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ñ–æ—Ä–º–∞—Ç—Ç–µ—Ä–∞ –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞
formatter = logging.Formatter(
    "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)

# –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
console_handler = logging.StreamHandler()
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

# –§–∞–π–ª–æ–≤—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
file_handler = logging.FileHandler("agent.log", encoding='utf-8')
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)

# –ü–æ–ª—É—á–∞–µ–º OpenAI API –∫–ª—é—á
openai_api_key = os.getenv("OPENAI_API_KEY")

if not openai_api_key:
    logger.error("OPENAI_API_KEY not found in environment variables")
    raise ValueError("OPENAI_API_KEY is required")

# -------------------- Video Helper Functions --------------------
def encode_frame_to_base64(frame: rtc.VideoFrame) -> str:
    """–ö–æ–¥–∏—Ä—É–µ—Ç –≤–∏–¥–µ–æ –∫–∞–¥—Ä –≤ base64 JPEG —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º —Å–∂–∞—Ç–∏–µ–º"""
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º LiveKit's encode —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è GPT-4o-mini
        image_bytes = encode(
            frame,
            EncodeOptions(
                format="JPEG",
                quality=80,  # –ë–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Ä–∞–∑–º–µ—Ä–∞
                resize_options=ResizeOptions(
                    width=1024,
                    height=1024,
                    strategy="scale_aspect_fit"  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏
                )
            )
        )
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º data URL
        base64_str = base64.b64encode(image_bytes).decode('utf-8')
        return f"data:image/jpeg;base64,{base64_str}"
        
    except Exception as e:
        logger.error(f"‚ùå [VIDEO ENCODE] Error encoding frame: {e}")
        return None

# -------------------- Agent Class with Video --------------------
class OpenAIAssistant(Agent):
    """–ì–æ–ª–æ—Å–æ–≤–æ–π –ø–æ–º–æ—â–Ω–∏–∫ —Å OpenAI, —Ç—Ä–µ–º—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –∏ —Ä—É—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –≤–∏–¥–µ–æ"""
    
    def __init__(self):
        super().__init__(
            instructions=(
                "You are a helpful voice assistant with access to weather information, web search, email sending, and live video from the user's camera. "
                "ALWAYS respond in English only, regardless of what language the user speaks. "
                "You understand all languages but respond ONLY in English. "
                "Do NOT mention the language issue - just answer naturally in English. "
                "When users ask about weather, use the get_weather tool and provide the exact information returned. "
                "When users ask for information you don't know, use the search_web tool to find current information. "
                "When users ask to send email, use the send_email tool with the information they provide. "
                "When users ask about what you see or show you something, describe what you can see in the video. "
                "You have access to live video from the user's camera and can see what they are showing you. "
                "Be specific and detailed when describing what you see - count objects, read text, describe colors and positions. "
                "Do NOT make up information - only use data from your tools or what you actually see in the video. "
                "Be clear, concise, and direct. Do NOT add phrases like 'If you have any other questions' or 'Let me know if you need more help' - just give the information requested."
            ),
            # –í—Å–µ —Ç—Ä–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
            tools=[get_weather, search_web, send_email],
        )
        
        # –í–∏–¥–µ–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞
        self._latest_frame = None
        self._video_stream = None
        self._frame_count = 0
        self._last_frame_time = 0
        self._video_tasks = []  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è async –∑–∞–¥–∞—á
        
        logger.info("‚úÖ OpenAI Assistant agent initialized with weather, search, email tools and manual video processing")

    async def on_enter(self):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –∫–æ–≥–¥–∞ –∞–≥–µ–Ω—Ç –≤—Ö–æ–¥–∏—Ç –≤ –∫–æ–º–Ω–∞—Ç—É"""
        logger.info("üöÄ [AGENT] Agent entered room, setting up video processing...")
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≤–∏–¥–µ–æ
        await self._setup_video_processing()
        
        # –¢–∞–∫–∂–µ —Å–ª–µ–¥–∏–º –∑–∞ –Ω–æ–≤—ã–º–∏ –≤–∏–¥–µ–æ —Ç—Ä–µ–∫–∞–º–∏
        room = get_job_context().room
        
        @room.on("track_subscribed")
        def on_track_subscribed(track: rtc.Track, publication: rtc.RemoteTrackPublication, participant: rtc.RemoteParticipant):
            if track.kind == rtc.TrackKind.KIND_VIDEO:
                logger.info(f"üìπ [VIDEO] New video track subscribed from {participant.identity}")
                asyncio.create_task(self._setup_video_stream(track))

    async def _setup_video_processing(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç—Ä–µ–∫–æ–≤"""
        try:
            room = get_job_context().room
            
            # –ò—â–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –≤–∏–¥–µ–æ —Ç—Ä–µ–∫–∏
            for participant in room.remote_participants.values():
                logger.info(f"üë§ [PARTICIPANT] Checking {participant.identity} for video tracks")
                
                for publication in participant.track_publications.values():
                    track = publication.track
                    if track and track.kind == rtc.TrackKind.KIND_VIDEO:
                        logger.info(f"üìπ [VIDEO] Found existing video track from {participant.identity}")
                        await self._setup_video_stream(track)
                        return  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π –≤–∏–¥–µ–æ —Ç—Ä–µ–∫
            
            logger.info("üìπ [VIDEO] No existing video tracks found, waiting for new ones...")
            
        except Exception as e:
            logger.error(f"‚ùå [VIDEO SETUP] Error setting up video processing: {e}")

    async def _setup_video_stream(self, track: rtc.Track):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ—Ç–æ–∫–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –≤–∏–¥–µ–æ —Ç—Ä–µ–∫–∞"""
        try:
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π –ø–æ—Ç–æ–∫ –µ—Å–ª–∏ –µ—Å—Ç—å
            if self._video_stream:
                logger.info("üìπ [VIDEO] Closing previous video stream")
                self._video_stream.close()
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –ø–æ—Ç–æ–∫
            self._video_stream = rtc.VideoStream(track)
            logger.info("üìπ [VIDEO] Created new video stream")
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –∫–∞–¥—Ä–æ–≤
            task = asyncio.create_task(self._process_video_frames())
            self._video_tasks.append(task)
            task.add_done_callback(lambda t: self._video_tasks.remove(t) if t in self._video_tasks else None)
            
            logger.info("‚úÖ [VIDEO] Video stream processing started")
            
        except Exception as e:
            logger.error(f"‚ùå [VIDEO STREAM] Error setting up video stream: {e}")

    async def _process_video_frames(self):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Ö–æ–¥—è—â–∏–µ –≤–∏–¥–µ–æ –∫–∞–¥—Ä—ã —Å –ø—Ä–æ–ø—É—Å–∫–æ–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ tokens"""
        try:
            logger.info("üé¨ [VIDEO FRAMES] Starting frame processing loop with frame skipping")
            
            async for event in self._video_stream:
                try:
                    frame = event.frame
                    self._frame_count += 1
                    
                    # –≠–ö–û–ù–û–ú–ò–Ø TOKENS: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –∫–∞–∂–¥—ã–π 10-–π –∫–∞–¥—Ä
                    if self._frame_count % 10 != 0:
                        continue
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π 30-–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∫–∞–¥—Ä —á—Ç–æ–±—ã –Ω–µ —Å–ø–∞–º–∏—Ç—å
                    if self._frame_count % 30 == 0:
                        logger.info(f"üì∏ [VIDEO FRAME] Processed {self._frame_count} frames, latest: {frame.width}x{frame.height}")
                    
                    # –ö–æ–¥–∏—Ä—É–µ–º –∫–∞–¥—Ä –≤ base64
                    encoded_frame = encode_frame_to_base64(frame)
                    
                    if encoded_frame:
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∫–∞–¥—Ä –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —á–∞—Ç–µ
                        self._latest_frame = encoded_frame
                        
                        # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∫–∞–¥—Ä–∞
                        import time
                        self._last_frame_time = time.time()
                        
                        # –õ–æ–≥–∏—Ä—É–µ–º —É—Å–ø–µ—à–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
                        if self._frame_count % 30 == 0:
                            logger.info(f"‚úÖ [VIDEO FRAME] Successfully encoded frame {self._frame_count} (skipping 9/10 frames)")
                    else:
                        logger.warning(f"‚ö†Ô∏è [VIDEO FRAME] Failed to encode frame {self._frame_count}")
                        
                except Exception as e:
                    logger.error(f"‚ùå [VIDEO FRAME] Error processing frame {self._frame_count}: {e}")
                    
        except Exception as e:
            logger.error(f"‚ùå [VIDEO FRAMES] Video frame processing loop ended: {e}")
        
        logger.info("üõë [VIDEO FRAMES] Frame processing loop ended")

    async def on_user_turn_completed(self, turn_ctx: ChatContext, new_message: ChatMessage) -> None:
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –∫–æ–≥–¥–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–∫–æ–Ω—á–∏–ª –≥–æ–≤–æ—Ä–∏—Ç—å - –¥–æ–±–∞–≤–ª—è–µ–º –≤–∏–¥–µ–æ –∫ –µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—é"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Å–≤–µ–∂–∏–π –≤–∏–¥–µ–æ –∫–∞–¥—Ä
            if self._latest_frame:
                import time
                frame_age = time.time() - self._last_frame_time
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–¥—Ä —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω —Å–≤–µ–∂–∏–π (–Ω–µ —Å—Ç–∞—Ä—à–µ 10 —Å–µ–∫—É–Ω–¥)
                if frame_age < 10:
                    # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º ImageContent
                    from livekit.agents.llm import ImageContent
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤–∏–¥–µ–æ –∫–∞–¥—Ä –∫ —Å–æ–æ–±—â–µ–Ω–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                    if hasattr(new_message, 'content') and isinstance(new_message.content, list):
                        new_message.content.append(ImageContent(image=self._latest_frame))
                        logger.info(f"üìπ [TURN COMPLETED] Added video frame to user message (frame age: {frame_age:.1f}s)")
                    else:
                        logger.warning("‚ö†Ô∏è [TURN COMPLETED] Could not add video - message content format unexpected")
                else:
                    logger.warning(f"‚ö†Ô∏è [TURN COMPLETED] Video frame too old ({frame_age:.1f}s), skipping")
            else:
                logger.info("üìπ [TURN COMPLETED] No video frame available to add to message")
                
        except Exception as e:
            logger.error(f"‚ùå [TURN COMPLETED] Error adding video to message: {e}")

    def __del__(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –∞–≥–µ–Ω—Ç–∞"""
        if self._video_stream:
            try:
                self._video_stream.close()
            except:
                pass

# -------------------- Entrypoint --------------------
async def entrypoint(ctx: JobContext):
    """–ì–ª–∞–≤–Ω–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è OpenAI –∞–≥–µ–Ω—Ç–∞ —Å –≤–∏–¥–µ–æ"""
    
    logger.info("üöÄ Starting OpenAI Assistant entrypoint with video support")
    
    # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –∫–æ–º–Ω–∞—Ç–µ —Å –∞–≤—Ç–æ–ø–æ–¥–ø–∏—Å–∫–æ–π –Ω–∞ –í–°–ï —Ç—Ä–µ–∫–∏ (–∞—É–¥–∏–æ + –≤–∏–¥–µ–æ)
    await ctx.connect(auto_subscribe=AutoSubscribe.SUBSCRIBE_ALL)
    logger.info(f"‚úÖ Connected to room: {ctx.room.name} with full auto-subscribe")
    
    # –°–æ–∑–¥–∞–µ–º –∞–≥–µ–Ω—Ç–∞ —Å –≤–∏–¥–µ–æ –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
    agent = OpenAIAssistant()
    
    # –û–ü–¢–ò–ú–ê–õ–¨–ù–ê–Ø —Å–µ—Å—Å–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ —Ä—É—á–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
    session = AgentSession(
        # VAD –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ —Ä–µ—á–∏
        vad=silero.VAD.load(),
        
        # OpenAI STT (Whisper) - –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û –¢–û–õ–¨–ö–û –ê–ù–ì–õ–ò–ô–°–ö–ò–ô!
        stt=openai.STT(
            language="en",  # –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û –∞–Ω–≥–ª–∏–π—Å–∫–∏–π - –Ω–∏–∫–∞–∫–æ–π –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è!
        ),
        
        # OpenAI LLM - GPT-4 —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π vision (–±–æ–ª—å—à–µ –ª–∏–º–∏—Ç–æ–≤)
        llm=openai.LLM(
            model="gpt-4o",  # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ GPT-4o —Å –±–æ–ª—å—à–∏–º–∏ –ª–∏–º–∏—Ç–∞–º–∏
            temperature=0.7,
        ),
        
        # OpenAI TTS –¥–ª—è –æ–∑–≤—É—á–∫–∏
        tts=openai.TTS(
            voice="alloy",
            speed=1.0,
        ),
        
        # –ù–ï–¢ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ video_sampler - –º—ã –¥–µ–ª–∞–µ–º —ç—Ç–æ –≤—Ä—É—á–Ω—É—é!
    )
    
    logger.info("‚úÖ Session created: Whisper STT (EN) + GPT-4o (vision + higher limits) + TTS + Manual Video Processing + 3 Tools")
    
    # ==========================================
    # –ü–†–ê–í–ò–õ–¨–ù–´–ï —Å–æ–±—ã—Ç–∏—è –∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–∞—Ö LiveKit
    # ==========================================
    
    @session.on("user_input_transcribed")
    def on_user_input_transcribed(event):
        """–ö–æ–≥–¥–∞ —Ä–µ—á—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞ STT"""
        transcript = getattr(event, 'transcript', 'No transcript')
        is_final = getattr(event, 'is_final', False)
        if is_final:
            logger.info(f"üë§ [USER FINAL] {transcript}")
            print(f"\nüë§ [USER] {transcript}")
        else:
            logger.debug(f"üë§ [USER PARTIAL] {transcript}")
    
    @session.on("conversation_item_added")
    def on_conversation_item_added(event):
        """–ö–æ–≥–¥–∞ —ç–ª–µ–º–µ–Ω—Ç –¥–æ–±–∞–≤–ª–µ–Ω –≤ –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞ (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ò–õ–ò –∞–≥–µ–Ω—Ç)"""
        item = getattr(event, 'item', None)
        if item:
            role = getattr(item, 'role', 'unknown')
            content = getattr(item, 'text_content', '') or str(getattr(item, 'content', ''))
            interrupted = getattr(item, 'interrupted', False)
            
            if role == "user":
                logger.info(f"üí¨ [CHAT USER] {content}")
                print(f"üí¨ [CHAT USER] {content}")
            elif role == "assistant":
                logger.info(f"üí¨ [CHAT ASSISTANT] {content}")
                print(f"üí¨ [CHAT ASSISTANT] {content}")
                print("-" * 60)
            
            if interrupted:
                logger.info(f"‚ö†Ô∏è [INTERRUPTED] {role} was interrupted")
    
    @session.on("speech_created")
    def on_speech_created(event):
        """–ö–æ–≥–¥–∞ –∞–≥–µ–Ω—Ç —Å–æ–∑–¥–∞–ª –Ω–æ–≤—É—é —Ä–µ—á—å"""
        logger.info("üîä [SPEECH CREATED] Agent is about to speak")
        print("üîä [ASSISTANT] Creating speech...")
    
    @session.on("agent_state_changed")
    def on_agent_state_changed(event):
        """–ö–æ–≥–¥–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ –∏–∑–º–µ–Ω–∏–ª–æ—Å—å"""
        old_state = getattr(event, 'old_state', 'unknown')
        new_state = getattr(event, 'new_state', 'unknown')
        logger.info(f"üîÑ [AGENT STATE] {old_state} -> {new_state}")
        print(f"üîÑ [AGENT] {old_state} -> {new_state}")
    
    @session.on("user_state_changed")  
    def on_user_state_changed(event):
        """–ö–æ–≥–¥–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–∑–º–µ–Ω–∏–ª–æ—Å—å"""
        old_state = getattr(event, 'old_state', 'unknown')
        new_state = getattr(event, 'new_state', 'unknown')
        logger.debug(f"üë§ [USER STATE] {old_state} -> {new_state}")
        
    # –°–û–ë–´–¢–ò–Ø –î–õ–Ø –û–¢–õ–ê–î–ö–ò –í–ò–î–ï–û
    @session.on("participant_connected")
    def on_participant_connected(event):
        """–ö–æ–≥–¥–∞ —É—á–∞—Å—Ç–Ω–∏–∫ –ø–æ–¥–∫–ª—é—á–∏–ª—Å—è"""
        participant = getattr(event, 'participant', None)
        if participant:
            logger.info(f"üîó [PARTICIPANT] Connected: {participant.identity}")
            print(f"üîó [PARTICIPANT] {participant.identity} connected")
    
    @session.on("track_subscribed") 
    def on_track_subscribed(event):
        """–ö–æ–≥–¥–∞ –ø–æ–¥–ø–∏—Å–∞–ª–∏—Å—å –Ω–∞ —Ç—Ä–µ–∫ (–∞—É–¥–∏–æ/–≤–∏–¥–µ–æ)"""
        track = getattr(event, 'track', None)
        participant = getattr(event, 'participant', None)
        if track and participant:
            track_kind = "video" if hasattr(track, 'kind') and str(track.kind) == "KIND_VIDEO" else "audio"
            logger.info(f"üìπ [TRACK] Subscribed to {track_kind} from {participant.identity}")
            print(f"üìπ [TRACK] Subscribed to {track_kind} from {participant.identity}")
        
    @session.on("function_tools_executed")
    def on_function_tools_executed(event):
        """–ö–æ–≥–¥–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã —Ñ—É–Ω–∫—Ü–∏–∏-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã"""
        logger.info("üõ†Ô∏è [TOOLS EXECUTED] Function tools completed")
        print("üõ†Ô∏è [TOOLS] Function executed - processing result...")
        
        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏
        if hasattr(event, 'function_calls') and event.function_calls:
            for i, call in enumerate(event.function_calls):
                function_name = getattr(call, 'function_name', 'unknown')
                result = getattr(call, 'result', 'no result')
                logger.info(f"üõ†Ô∏è [TOOL RESULT {i+1}] {function_name}: {str(result)[:200]}...")
                print(f"üõ†Ô∏è [TOOL {i+1}] {function_name}: {str(result)[:100]}...")
        
    @session.on("metrics_collected")
    def on_metrics_collected(event):
        """–ö–æ–≥–¥–∞ —Å–æ–±—Ä–∞–Ω—ã –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        # –û—Ç–∫–ª—é—á–∞–µ–º –≤—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫ - —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Å–ø–∞–º–∞
        pass
        
    @session.on("close")
    def on_session_close(event):
        """–ö–æ–≥–¥–∞ —Å–µ—Å—Å–∏—è –∑–∞–∫—Ä—ã–≤–∞–µ—Ç—Å—è"""
        logger.info("‚ùå [SESSION CLOSED] Agent session ended")
        print("‚ùå [SESSION] Closed")
        
    @session.on("error")
    def on_error(event):
        """–ö–æ–≥–¥–∞ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –æ—à–∏–±–∫–∞"""
        error = getattr(event, 'error', str(event))
        recoverable = getattr(error, 'recoverable', True) if hasattr(error, 'recoverable') else True
        logger.error(f"‚ùå [ERROR] {error} (recoverable: {recoverable})")
        print(f"‚ùå [ERROR] {error}")
    
    # ==========================================
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Å—Å–∏—é
    # ==========================================
    
    await session.start(
        agent=agent,
        room=ctx.room,
    )
    
    logger.info("‚úÖ Session started successfully with manual video processing")
    
    # –ù–∞—á–∞–ª—å–Ω–æ–µ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ
    try:
        await session.generate_reply(
            instructions="Say hello and introduce yourself as a helpful voice assistant that can see through the camera."
        )
        logger.info("‚úÖ Initial greeting generated")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not generate initial greeting: {e}")
        print("ü§ñ [ASSISTANT] Hello! I'm your voice assistant with video vision. How can I help you?")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    print("\n" + "="*80)
    print("ü§ñ [OPENAI ASSISTANT] Ready for conversation with video vision!")
    print("üìã [INFO] OpenAI Whisper STT (EN) + GPT-4o (vision + higher limits) + TTS + Manual Video + 3 Tools")
    print("üîç [VAD] Silero VAD for speech detection")
    print("üí∞ [COST] ~$0.15 per minute (higher but with much better limits)")
    print("‚ö° [LIMITS] GPT-4o has 5x higher rate limits than gpt-4o-mini")
    print("üåç [STT] Treats ALL speech as English (no language detection)")
    print("üìπ [VIDEO] Manual video processing - can see what you show (10:1 frame skipping for efficiency)")
    print("üõ†Ô∏è [TOOLS] Weather, Web Search, and Email sending available")
    print("üìù [LOGGING] All activity logged to agent.log and console")
    print("")
    print("üéØ [TEST COMMANDS] (ALL speech treated as English):")
    print("   ‚Ä¢ 'What's the weather in London?' ‚Üí weather tool") 
    print("   ‚Ä¢ 'Search for latest AI news' ‚Üí search tool")
    print("   ‚Ä¢ 'Send email to john@example.com about meeting' ‚Üí email tool")
    print("   ‚Ä¢ 'What do you see?' ‚Üí describes video from camera")
    print("   ‚Ä¢ 'How many fingers am I showing?' ‚Üí counts fingers in video")
    print("   ‚Ä¢ 'Can you read this text?' ‚Üí reads text from paper/screen")
    print("")
    print("üéÆ [CONTROLS] Speak into your microphone, press Ctrl+C to quit")
    print("="*80 + "\n")
    print("üéôÔ∏è [LISTENING] Start speaking now...")
    print("üìπ [VIDEO] Make sure camera is enabled in LiveKit Playground")
    print("üìπ [VIDEO] Manual video processing will start automatically")
    
    # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –∫–æ–º–Ω–∞—Ç—ã
    try:
        room_participants = len(ctx.room.remote_participants)
        logger.info(f"üè† [ROOM] {room_participants} remote participants")
        print(f"üè† [ROOM] {room_participants} remote participants")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤–∏–¥–µ–æ —Ç—Ä–µ–∫–æ–≤
        for participant in ctx.room.remote_participants.values():
            video_tracks = [pub for pub in participant.track_publications.values() 
                          if hasattr(pub.track, 'kind') and str(pub.track.kind) == "KIND_VIDEO"]
            logger.info(f"üìπ [PARTICIPANT] {participant.identity} has {len(video_tracks)} video tracks")
            print(f"üìπ [PARTICIPANT] {participant.identity} has {len(video_tracks)} video tracks")
            
    except Exception as e:
        logger.debug(f"‚ö†Ô∏è [DEBUG] Room info error: {e}")
    
    # –ë–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π —Ü–∏–∫–ª –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–∞
    try:
        while True:
            await asyncio.sleep(1)
    except KeyboardInterrupt:
        logger.info("üëã [SHUTDOWN] OpenAI Assistant shutting down...")
        print("\nüëã [ASSISTANT] Goodbye!")

# -------------------- Main --------------------
if __name__ == "__main__":
    logger.info("üöÄ Starting OpenAI Assistant LiveKit agent application with manual video processing")
    cli.run_app(
        WorkerOptions(
            entrypoint_fnc=entrypoint
        )
    )